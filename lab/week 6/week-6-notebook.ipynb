{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Starting web crawler...\n",
						"\n",
						"Processing page: https://scrapeme.live/shop/\n",
						"Extracted product: Bulbasaur\n",
						"Extracted product: Ivysaur\n",
						"Extracted product: Venusaur\n",
						"Extracted product: Charmander\n",
						"Extracted product: Charmeleon\n",
						"Extracted product: Charizard\n",
						"Extracted product: Squirtle\n",
						"Extracted product: Wartortle\n",
						"Extracted product: Blastoise\n",
						"Extracted product: Caterpie\n",
						"Extracted product: Metapod\n",
						"Extracted product: Butterfree\n",
						"Extracted product: Weedle\n",
						"Extracted product: Kakuna\n",
						"Extracted product: Beedrill\n",
						"Extracted product: Pidgey\n",
						"\n",
						"Processing page: https://scrapeme.live/shop/page/2/\n",
						"Extracted product: Pidgeotto\n",
						"Extracted product: Pidgeot\n",
						"Extracted product: Rattata\n",
						"Extracted product: Raticate\n",
						"Extracted product: Spearow\n",
						"Extracted product: Fearow\n",
						"Extracted product: Ekans\n",
						"Extracted product: Arbok\n",
						"Extracted product: Pikachu\n",
						"Extracted product: Raichu\n",
						"Extracted product: Sandshrew\n",
						"Extracted product: Sandslash\n",
						"Extracted product: Nidorina\n",
						"Extracted product: Nidoqueen\n",
						"Extracted product: Nidorino\n",
						"Extracted product: Nidoking\n",
						"\n",
						"Processing page: https://scrapeme.live/shop/page/3/\n",
						"Extracted product: Clefairy\n",
						"Extracted product: Clefable\n",
						"Extracted product: Vulpix\n",
						"Extracted product: Ninetales\n",
						"Extracted product: Jigglypuff\n",
						"Extracted product: Wigglytuff\n",
						"Extracted product: Zubat\n",
						"Extracted product: Golbat\n",
						"Extracted product: Oddish\n",
						"Extracted product: Gloom\n",
						"Extracted product: Vileplume\n",
						"Extracted product: Paras\n",
						"Extracted product: Parasect\n",
						"Extracted product: Venonat\n",
						"Extracted product: Venomoth\n",
						"Extracted product: Diglett\n",
						"\n",
						"Processing page: https://scrapeme.live/shop/page/4/\n",
						"Extracted product: Dugtrio\n",
						"Extracted product: Meowth\n",
						"Extracted product: Persian\n",
						"Extracted product: Psyduck\n",
						"Extracted product: Golduck\n",
						"Extracted product: Mankey\n",
						"Extracted product: Primeape\n",
						"Extracted product: Growlithe\n",
						"Extracted product: Arcanine\n",
						"Extracted product: Poliwag\n",
						"Extracted product: Poliwhirl\n",
						"Extracted product: Poliwrath\n",
						"Extracted product: Abra\n",
						"Extracted product: Kadabra\n",
						"Extracted product: Alakazam\n",
						"Extracted product: Machop\n",
						"\n",
						"Processing page: https://scrapeme.live/shop/page/5/\n",
						"Extracted product: Machoke\n",
						"Extracted product: Machamp\n",
						"Extracted product: Bellsprout\n",
						"Extracted product: Weepinbell\n",
						"Extracted product: Victreebel\n",
						"Extracted product: Tentacool\n",
						"Extracted product: Tentacruel\n",
						"Extracted product: Geodude\n",
						"Extracted product: Graveler\n",
						"Extracted product: Golem\n",
						"Extracted product: Ponyta\n",
						"Extracted product: Rapidash\n",
						"Extracted product: Slowpoke\n",
						"Extracted product: Slowbro\n",
						"Extracted product: Magnemite\n",
						"Extracted product: Magneton\n",
						"\n",
						"Extraction complete. 80 products saved to products.csv\n"
					]
				}
			],
			"source": [
				"import requests\n",
				"from bs4 import BeautifulSoup\n",
				"import csv\n",
				"from queue import PriorityQueue\n",
				"import time\n",
				"import random\n",
				"\n",
				"class ProductCrawler:\n",
				"    def __init__(self, start_url, max_pages=50):\n",
				"        \"\"\"Initialize the crawler with starting URL and page limit\"\"\"\n",
				"        self.urls = PriorityQueue()\n",
				"        self.urls.put((1, start_url))  # Priority 1 for the start URL\n",
				"        self.visited_urls = set()\n",
				"        self.products = []\n",
				"        self.max_pages = max_pages\n",
				"        self.request_delay = 1  # Base delay in seconds\n",
				"        \n",
				"    def extract_products_from_page(self, url):\n",
				"        \"\"\"Extract product information from a given page\"\"\"\n",
				"        try:\n",
				"            response = requests.get(url)\n",
				"            soup = BeautifulSoup(response.content, \"html.parser\")\n",
				"            \n",
				"            # Find all product elements on the page\n",
				"            products_on_page = soup.find_all(\"li\", class_=\"product\")\n",
				"            \n",
				"            for product_element in products_on_page:\n",
				"                product = {\n",
				"                    'url': product_element.find('a')['href'],\n",
				"                    'image': product_element.find('img')['src'],\n",
				"                    'title': product_element.find('h2', class_='woocommerce-loop-product__title').text.strip()\n",
				"                }\n",
				"                \n",
				"                # Extract price if available\n",
				"                price_element = product_element.find('span', class_='woocommerce-Price-amount')\n",
				"                product['price'] = price_element.text.strip() if price_element else 'Price not available'\n",
				"                \n",
				"                self.products.append(product)\n",
				"                print(f\"Extracted product: {product['title']}\")\n",
				"            \n",
				"            # Find pagination links and add them with high priority\n",
				"            next_page = soup.find('a', class_='next')\n",
				"            if next_page and next_page['href'] not in self.visited_urls:\n",
				"                self.urls.put((1, next_page['href']))  # Priority 1 for pagination\n",
				"\n",
				"        except Exception as e:\n",
				"            print(f\"Error processing {url}: {e}\")\n",
				"\n",
				"    def crawl(self):\n",
				"        \"\"\"Main crawling method\"\"\"\n",
				"        pages_processed = 0\n",
				"        \n",
				"        while not self.urls.empty() and pages_processed < self.max_pages:\n",
				"            priority, current_url = self.urls.get()\n",
				"            \n",
				"            if current_url in self.visited_urls:\n",
				"                continue\n",
				"                \n",
				"            print(f\"\\nProcessing page: {current_url}\")\n",
				"            self.extract_products_from_page(current_url)\n",
				"            self.visited_urls.add(current_url)\n",
				"            \n",
				"            pages_processed += 1\n",
				"            \n",
				"            # Add random delay between requests\n",
				"            delay = self.request_delay + random.uniform(0.5, 2.0)\n",
				"            time.sleep(delay)\n",
				"\n",
				"    def save_to_csv(self, filename='products.csv'):\n",
				"        \"\"\"Save extracted products to CSV file\"\"\"\n",
				"        try:\n",
				"            with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
				"                writer = csv.DictWriter(csv_file, fieldnames=['url', 'image', 'title', 'price'])\n",
				"                writer.writeheader()\n",
				"                for product in self.products:\n",
				"                    writer.writerow(product)\n",
				"            print(f\"\\nExtraction complete. {len(self.products)} products saved to {filename}\")\n",
				"        except Exception as e:\n",
				"            print(f\"Error saving to CSV: {e}\")\n",
				"\n",
				"def main():\n",
				"    start_url = \"https://scrapeme.live/shop/\"\n",
				"    crawler = ProductCrawler(start_url, max_pages=5)\n",
				"    \n",
				"    print(\"Starting web crawler...\")\n",
				"    crawler.crawl()\n",
				"    crawler.save_to_csv()\n",
				"\n",
				"if __name__ == \"__main__\":\n",
				"    main()"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.12.4"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
